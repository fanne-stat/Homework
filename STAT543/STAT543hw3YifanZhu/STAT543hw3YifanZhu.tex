
	\documentclass{article}
	\usepackage{amsmath,amssymb}
	\usepackage[inline]{enumitem}
	\usepackage{blindtext}
	\usepackage{booktabs}
	\usepackage{graphicx}
	\usepackage{xcolor}
	\usepackage[vmargin = 1.5in, top = 1in, bottom = 1.2in, letterpaper]{geometry}
	\usepackage{listings}
	\usepackage{courier}
	\usepackage{multicol}
	\usepackage{multirow}
	\usepackage{bm}
	\lstset{
	basicstyle = \small\tt,
	keywordstyle = \tt\color{blue},
	commentstyle = \it\color[cmyk]{1,0,1,0},
	stringstyle = \tt\color[RGB]{128,0,0},
	%frame = single,
	backgroundcolor = \color[RGB]{245,245,244},
	breaklines,
	extendedchars = false,
	xleftmargin = 2em,
	xrightmargin = 2em,
	aboveskip = 1em,
	tabsize = 4,
	showspaces = false
	}
	\begin{document}
	
	% \newfontfamily\courier{Courier New}

	
	\title{STAT 543 Homework 3}
	\author{Yifan Zhu}
	\maketitle
	
	\begin{enumerate}[leftmargin = 0 em, label = \arabic*., font = \bfseries]
	\item 
		The posterior pdf is
		\[f(\theta | \tilde{x}) \propto f(\tilde{x}| \theta) \pi(\theta) = \prod_{i=1}^n \theta^{x_i} (1 - \theta)^{1-x_i} = \theta^{\sum_{i=1}^n x_i} (1 - \theta)^{n - \sum_{i=1}^n x_i}\]

		Thus 
		\[E(L(t,\theta)| \tilde{x}) \propto \int_{0}^1 \theta^{\sum_{i=1}^n x_i} (1 - \theta)^{n - \sum_{i=1}^n x_i} \frac{(t - \theta)^2}{\theta (1 - \theta)} \mathrm{d}\theta = \int_{0}^1 \theta^{\sum_{i=1}^n x_i -1} (1 - \theta)^{n - \sum_{i=1}^n x_i - 1} (t - \theta)^2 \mathrm{d}\theta\]

		If we take the $(t - \theta)^2$ above as a loss function and $\theta^{\sum_{i=1}^n - 1} (1 - \theta)^{n - \sum_{i=1}^n - 1}$ as a posterior, then to minimize the above equation, the $t$ we chose is the expectation of a Beta distribution, where $\alpha = \sum_{i=1}^n x_i$ and $\beta = n - \sum_{i=1}^n x_i$. Thus
		\[t = \frac{\alpha}{\alpha + \beta} = \frac{\sum_{i=1}^n x_i}{\sum_{i=1}^n x_i + n - \sum_{i=1}^n x_i} = \frac{\sum_{i=1}^n x_i}{n}\]
		The Bayes estimator $T_0 = \frac{\sum_{i=1}^n X_i}{n}$.
 	
 	\item 
 	\begin{enumerate}
 		\item 
 		As $X_1, X_2, \cdots , X_n$ are iid $N(\theta, 1)$, then $\bar{X}_n \sim N(\theta , 1/n)$, hence
 		\begin{align*}
 		E(T) &= E((\bar{X}_n)^2 - \frac{1}{n}) \\
 		&= E((\bar{X}_n)^2) - \frac{1}{n} \\
 		&= Var((\bar{X}_n)^2) + (E(\bar{X}_n))^2 - \frac{1}{n} \\
 		&= \frac{1}{n} + \theta^2 - \frac{1}{n} = \theta^2
 		\end{align*}
 		Thus $T$ is an unbiased estimator.

 		$\sqrt{n} (\bar{X}_n - \theta) = Y \Rightarrow \bar{X}_n = Y/\sqrt{n} + \theta \Rightarrow (\bar{X}_n )^2 = Y^2/n + 2 \theta Y / \sqrt{n} + \theta^2 $. Here we know $Y \sim N(0,1)$ and $Y^2 \sim \chi_{1}^2$. Thus $Var_{\theta} (Y^2) = 2$ and $Var_{\theta} (Y) = 1$.

 		Also, for $Y\sim N(0,1)$, 
 		$$Cov_{\theta}(Y^2, Y) = E(Y^3) - E(Y^2)E(Y) = E(Y^3) = \int_{-\infty}^{\infty} y^3 \frac{1}{\sqrt{2 \pi}} \mathrm{e}^{-\frac{y^2}{2}}\mathrm{d}y = 0$$
 		 As the function in the integral is an odd function and the integral interval is symmetric about 0, thus the integral is 0.

 		 Then
 		 \begin{align*}
 		 Var_{\theta}(T) & = Var_{\theta}((\bar{X}_n)^2 - \frac{1}{n})\\
 		 & = Var_{\theta}((\bar{X}_n)^2)\\
 		 & = Var_{\theta} (Y^2 / n + 2 \theta Y /\sqrt{n} + \theta^2)\\
 		 & = Var_{\theta} (Y^2 / n + 2 \theta Y /\sqrt{n})\\
 		 & = \frac{1}{n^2} Var_{\theta} (Y) + \frac{4 \theta^2}{n} Var_{\theta} (Y) + \frac{4 \theta}{n \sqrt{n}} Cov_{\theta}(Y^2, Y)\\
 		 & = \frac{2}{n^2} + \frac{4 \theta^2}{n}
 		 \end{align*}
 		
 		\item 
 		\begin{align*}
 		& \gamma(\theta) = \theta^2 \Rightarrow \gamma' (\theta) = 2 \theta\\
 		& \frac{\mathrm{d}}{\mathrm{d}\theta}\log f(X_1 | \theta)\\
 		=& \frac{\mathrm{d}}{\mathrm{d}\theta} \log \left(\frac{1}{\sqrt{2 \pi}} \mathrm{e}^{-\frac{(X_1 - \theta)^2}{2}}\right)\\
 		& = \frac{\mathrm{d}}{{\mathrm{d}\theta}} \left( \log \frac{1}{\sqrt{2 \pi}} - \frac{(X_1 - \theta)^2}{2}\right)\\
 		& = X_1 - \theta
 		\end{align*}

 		Then 
 		\[I_1 (\theta)  = E_{\theta} ((X_1 - \theta)^2) = Var_{\theta}(X_1) = 1 \Rightarrow I_n (\theta) = n I_1 (\theta) = n\].

 		Thus 
 		\[CRLB = \frac{(\gamma'(\theta))^2}{I_{n}(\theta)} = \frac{4 \theta^2}{n}\]

 		\item 
 		\[Var_{\theta}(T) - CRLB = \frac{4 \theta^2}{n} + \frac{2}{n^2} - \frac{4 \theta^2}{n} = \frac{2}{n^2} > 0 , \, \forall \theta \in \Theta\]
 		Hence $Var_{\theta}(T) > CRLB$ for all $\theta$.
 		
 	\end{enumerate}

 	\item 
 	\begin{enumerate}
 		\item
 	
 	$\bar{X}_n | \theta \sim N(\theta, \frac{9}{25})$,
 	\begin{align*}
 	f(\bar{x} , \theta) & = f(\bar{x}|\theta) f(\theta)\\
 	& = \frac{1}{\sqrt{2 \pi} \frac{3}{5}} \mathrm{e}^{-\frac{(\bar{x} - \theta)^2}{2 \cdot \frac{9}{25}}} \frac{1}{\sqrt{2 \pi} 4} \mathrm{e}^{- \frac{(\theta - 10)^2}{2 \cdot 16}}\\
 	& = \frac{5}{24 \pi} \mathrm{e}^{-\frac{25(\bar{x} - \theta)^2}{18} - \frac{(\theta - 10)^2}{32}}, \, (\bar{x}, \theta) \in \mathbb{R}^2
 	\end{align*}
 	
 	
 	\item 
 	For $X_i | \theta \sim N(\theta, \sigma^2)$ and $\theta \sim N(\mu , \tau^2)$, we have 
 	\[m(\bar{x}) \sim N(\mu, \sigma_n^2 + \tau^2)\]
 	Here $\sigma_n^2 = \frac{\sigma^2}{n}$.

 	For this question, $\sigma_n^2 = \frac{9}{25}, \tau^2 = 16, \mu  = 10$, then $\sigma_n^2 + \tau^2 = \frac{9}{25} + 16 = \frac{409}{25}$. Then
 	\[m(\bar{x}) \sim N(10, \frac{409}{25})\]


 	\item 
 	For $X_i | \theta \sim N(\theta, \sigma^2)$ and $\theta \sim N(\mu , \tau^2)$, we have 
 	\[\theta | \bar{x} \sim N\left(\frac{\tau^2}{\tau^2 + \sigma_n^2} \bar{x} + \frac{\sigma_n^2}{\tau^2 + \sigma_n^2} \mu , \frac{\tau^2 \sigma_n^2}{\tau^2 + \sigma_n^2}\right)\]
 	Then 
 	\begin{align*}
 	&E(\theta | \bar{x} = 18) = \frac{\tau^2}{\tau^2 + \sigma_n^2} \bar{x} + \frac{\sigma_n^2}{\tau^2 + \sigma_n^2} \mu = \frac{16}{16 + \frac{9}{25}} \cdot 18 + \frac{\frac{9}{25}}{16 + \frac{9}{25}} \cdot 10 = \frac{7290}{409}\\
 	& Var(\theta | \bar{x} = 18) = \frac{\tau^2 \sigma_n^2}{\tau^2 + \sigma_n^2} = \frac{16 \cdot \frac{9}{25}}{16 + \frac{9}{25}} = \frac{144}{409}
 	\end{align*}
 	 	\end{enumerate}

 	 	\item 
 	 	\begin{enumerate}
 	 		\item 
 	 	\begin{align*}
 	 	f(\tilde{x}|\lambda) & = f(x_1 | \lambda) f(x_2 | \lambda) f(x_3 | \lambda)\\
 	 	& = \frac{\lambda^{x_1}}{x_1 !} \mathrm{e}^{- \lambda} \frac{\lambda^{x_2}}{x_2 !} \mathrm{e}^{- \lambda} \frac{\lambda^{x_3}}{x_3 !} \mathrm{e}^{- \lambda}\\
 	 	& = \frac{\lambda^{x_1 + x_2 + x_3}}{x_1 ! x_2 ! x_3 !} \mathrm{e}^{- 3 \lambda}\\
 	 	&\\
		f(\lambda) & =  \lambda^{1-1} \frac{\mathrm{e}^{- \lambda / 2}}{\Gamma(1) 2} = \frac{1}{2} \mathrm{e}^{- \lambda / 2}\\
		&\\
		f(\tilde{x}, \lambda) &= f(\tilde{x}|\lambda)f(\lambda)\\
		& = \frac{\lambda^{x_1 + x_2 + x_3}}{2 x_1 ! x_2! x_3!} \mathrm{e}^{-\frac{7}{2}\lambda}\\
		&\\
		m(\tilde{x})& = \int_{0}^\infty \frac{\lambda^{x_1 + x_2 + x_3}}{2 x_1 ! x_2 ! x_3 !} \mathrm{e}^{- \frac{7}{2}\lambda} \mathrm{d}\lambda\\
		& = \frac{1}{2 x_1 ! x_2 ! x_3 !} \int_{0}^\infty \lambda^{x_1 + x_2 + x_3} \mathrm{e}^{-\frac{7}{2} \lambda} \mathrm{d}\lambda\\
		& = \frac{1}{2 x_1 ! x_2 ! x_3 !} \int_{0}^\infty (\frac{2}{7})^{x_1 + x_2 + x_3 + 1} t^{x_1 + x_2 + x_3 ＋１ - 1} \mathrm{e}^{-t}\mathrm{d}t\\
		& = \frac{2^{x_1 + x_2 + x_3}}{7^{x_1 + x_2 + x_3 + 1} x_1 ! x_2 ! x_3 !} \Gamma(x_1 + x_2 + x_3 + 1) 
 	 	\end{align*}
 	 	Thus
 	 	\[f(\lambda | \tilde{x}) = \frac{7^{x_1 + x_2 + x_3 + 1} x_1!x_2 !x_3!}{2^{x_1 + x_2 + x_3 } \Gamma(x_1 + x_2 + x_3 + 1)} \frac{\lambda^{x_1 + x_2 + x_3}}{2 x_1 ! x_2 ! x_3 !} \mathrm{e}^{- \frac{7}{2}\lambda} = \frac{1}{(2/7)^{x_1 + x_2 + x_3 + 1}} \lambda^{x_1 + x_2 + x_3 + 1 -1} \mathrm{e}^{-\frac{\lambda}{2/7}}\]

 	 	Thus 
 	 	\[\lambda | \tilde{x} \sim Gamma(x_1 + x_2 + x_3  + 1, \frac{2}{7})\]

 	 	\item 
 	 	\begin{align*}
 	 	& E(\lambda| \tilde{x}) = \frac{2}{7} (x_1 + x_2 + x_3 + 1)\\
 	 	& Var(\lambda | \tilde{x}) = \frac{4}{49}(x_1 + x_2 + x_3 + 1)
 	 	\end{align*}
 	 	
 	 	\end{enumerate}
 	 	
 	 	\item 
 	 	\begin{enumerate}
 	 		\item 
 	 		For double exponential distribution, $E(X_i) = \mu$, then
 	 		\[E(\sum_{i=1}^n a_i X_i) = \sum_{i=1}^n a_i E(X_i) = \sum_{i=1}^n a_i \mu = \mu\]
 	 		Thus it is unbiased estimator.

 	 		\item 
 	 		\[\sum_{i=1}^n (a_i - \bar{a})^2 = \sum_{i=1}^n (a_i^2 - 2 \bar{a} a_i + (\bar{a})^2) = \sum_{i=1}^n a_i^2 - 2 \bar{a} \sum_{i=1}^n a_i + n (\bar{a})^2 = \sum_{i=1}^n a_i^2 - n (\bar{a})^2\]
 	 		Then 
 	 		\[\sum_{i=1}^n a_i^2 = \sum_{i=1}^n (a_i - \bar{a})^2 + n (\bar{a})^2\]
 	 		\begin{align*}
 	 		Var(\sum_{i=1}^n a_i X_i)& = \sum_{i=1}^n a_i^2 Var(X_i) = \sum_{i=1}^n a_i^2 2 \sigma^2\\
 	 		& = 2 \sigma^2 \sum_{i=1}^n a_i^2\\
 	 		& = 2 \sigma^2 \left[\sum_{i=1}^n (a_i - \bar{a})^2 + n (\bar{a})^2\right]\\
 	 		& = 2 \sigma^2 \left[\sum_{i=1}^n (a_i - \frac{1}{n})^2 + \frac{1}{n}\right]\\
 	 		& \geq \frac{2 \sigma^2}{n}
 	 		\end{align*}
 	 		The equation holds only when $a_i = \frac{1}{n}$. Thus for the case $n = 3$, among all the linear unbiased estimator $\sum_{i=1}^3 \frac{1}{3} X_i$ has the smallest variance.
 	 		
 	 	\end{enumerate}

 	 	\item 
 	 	\begin{enumerate}
 	 		\item 
 	 		\begin{align*}
 	 		\log L(\theta) &= |x| \log \frac{\theta}{2} + (1 - |x|) \log(1 - \theta)\\
 	 		\frac{\mathrm{d}}{\mathrm{d}\theta} \log L(\theta) &= \frac{1}{\theta} |x| - (1 - |x|) \frac{1}{1 - \theta}\\
 	 		& = \frac{|x|(1 - \theta) - (1 - |x|)\theta}{\theta(1 - \theta)}\\
 	 		& = \frac{|x| - \theta}{\theta(1 - \theta)}
 	 		\end{align*}
 	 		Hence
 	 		\[\frac{\mathrm{d}}{\mathrm{d}\theta} \log L(\theta) \bigg|_{\hat{\theta}} = 0 \Rightarrow \hat{\theta} = |x|\]
 	 		We also have 
 	 		\[\frac{\mathrm{d^2}}{\mathrm{d}\theta^2} \log L(\theta) = - \frac{1}{\theta^2} |x| - (1- |x|) \frac{1}{(1 - \theta)^2} < 0\]
 	 		Then $\hat{\theta} = |X|$.
 	 		\item 
 	 		\[E(T(X)) = 2 P(X = 1) = 2 \frac{\theta}{2} = \theta\]
 	 		Thus $T(X)$ is unbiased.

 	 		\item 
 	 		\begin{align*}
 	 		MSE_{T(X)} &= E((T(X) - \theta)^2)\\
 	 		& = (2 - \theta)^2 \frac{\theta}{2} + (0 - \theta)^2 (1 - \frac{\theta}{2})\\
 	 		& = (\theta^2 - 4 \theta + 4) \frac{\theta}{2} + \theta^2 (1 - \frac{\theta}{2})\\
 	 		& = - \theta^2 + 2 \theta 
 	 		\end{align*}

 	 		Take another estimator $T_1 (X) = |X|$. Then
 	 		\begin{align*}
 	 		MSE_{T_1 (X)}& = E((T_1 (X) - \theta)^2)\\
 	 		& = (1 - \theta)^2 \frac{\theta}{2} + \theta^2 (1 - \frac{\theta}{2})\\
 	 		& = (\theta^2 - 2 \theta + 1) \frac{\theta}{2} + \theta^2 - \frac{\theta^3}{2}\\
 	 		& = \frac{\theta}{2} 	 		
 	 		\end{align*}

 	 		\[MSE_{T_1(X)} - MSE_{T(X)} = \frac{\theta}{2} + \theta^2 - 2 \theta = \theta^2 - \frac{3}{2} \theta = \theta (\theta - \frac{3}{2}) \leq 0,\, \forall \theta \in [0,1]\]
 	 		And the less sign holds when $\theta \in (0,1]$. Hence $T_1 (X) = |X|$ is a better estimator in terms of MSE.
 	 		
 	 		
 	 		
 	 	\end{enumerate}
 	 	
 	 	
 	\end{enumerate}

	
	
	
	\end{document}