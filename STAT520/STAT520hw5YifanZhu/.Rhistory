h[1,1] <- sum(x.dat)*b[2,1]/(b[1,1]^2) +
crossprod(x.dat, x.cat)/((1-b[1,1])^2)
h[1,2] <- -sum(x.dat)/b[1,1]
h[2,1] <- h[1,2]
h[2,2] <- sum(xcum/((x.cat+(b[2,1]-1))^2))
h[2,2] <- h[2,2]-(xcum[1]/((b[2,1]-1)^2))
list(q=q, h=h)
}
# Modified Newton-Raphson Algorithm
mnr <- function(b, x.dat, x.cat,
maxit = 50, halving = 16,
conv = .000001)
{
check <- 1
iter <- 1
while(check > conv && iter < maxit+1) {
fold <- fun(b, x.dat, x.cat)
bold <- b
aa <- dfun(b, x.dat, x.cat)
hi <- solve(aa$h)
b <- bold + hi%*%aa$q
fnew <- fun(b, x.dat, x.cat)
hiter <- 1
while(fold-fnew > 0 && hiter < halving+1) {
b <- bold + 2.*((.5)^hiter)*hi%*%aa$q
fnew <- fun(b, x.dat, x.cat)
hiter <- hiter + 1
}
cat("\n", "Iteration = ", iter, " pi =", b[1,1],
" beta = ", b[2,1], "Log-likelihood", fnew)
iter <- iter + 1
check <- crossprod(bold-b,bold-b)/crossprod(bold,bold)
}
aa <- dfun(b, x.dat, x.cat)
hi <- solve(aa$h)
list(b = b, hi = hi, grad = aa$q)
}
#  The data in this example are
#  the smooth surface cavity data
#  considered in STAT 557.
#  Enter a list of counts for each
#  of the categories from zero
#  through K, the maximum number of
#  cavities seen in any one child.
#  All categories preceeding K must
#  be included, even if the observed
#  count is zero. There should be K+1
#  entries in the list of counts.
x.dat <- c(57,203,383,525,532,408,273,139,45,27,10,4,0,1,1)
mb<-2
#  Enter the category values
nc <- length(x.dat)
x.cat <- 0:(nc-1)
#  Compute the total count, mean,
#  and variance
n <- sum(x.dat)
m.x <- crossprod(x.dat, x.cat)/n
v.x <- (crossprod(x.dat, x.cat^2) - n*m.x^2)/n
#  Compute expected counts for
#  the Poisson model
m.p <- (n/m.x)*exp(-m.x)*
cumprod(m.x*(c(1,1:(length(x.dat)-1))^(-1)))
# Combine categories to keep expected
# counts above a lower bound
xcc <- combine(x.cat, x.dat, m.p, nc, mb)
# Compute the Pearson chi-squared
# test and p-value
x2p <- sum((xcc$xt-xcc$ept)^2/xcc$ept)
dfp <- length(xcc$ept) - 2
pvalp <- 1 - pchisq(x2p, dfp)
# Compute the G^2 statistic
g2<-0
for(i in 1:length(xcc$ept)) {
at<-0
if(xcc$xt[i] > 0)
{at<-2*xcc$xt[i]*log(xcc$xt[i]/xcc$ept[i])}
g2 <- g2+at}
pvalg <- 1 - pchisq(g2, dfp)
# Compute the Fisher Index of Dispersion test statistic
# The one-sided alternative is that the variance is
# larger than the mean.
fisherd <- n*v.x/m.x
dff <- n -1
pvalf <- 1 - pchisq(fisherd, dff)
# Print results
kk <- length(xcc$ept)
new2 <- matrix(c(xcc$cl, xcc$cu, xcc$xt, xcc$ept),
kk, 4)
cat("\n", "  Results for fitting the
Poisson distribution", "\n")
cat("\n", "Category Bounds  Count  Expected", "\n")
print(new2)
cat("\n", "      Pearson test = ", x2p)
cat("\n", "Degrees of freedom = ", dfp)
cat("\n", "           p-value = ", pvalp, "\n")
cat("   Likelihood ratio test =", g2, "\n")
cat("                      df =", dfp, "\n")
cat("                  p-value =", pvalg, "\n")
cat("\n","Fisher Index of Dispersion = ", fisherd)
cat("\n", "    Degrees of freedom = ", dff)
cat("\n", "               p-value = ", pvalf )
#  Begin computation for fitting the
#  negative binomial distribution
pi <- .95
beta <- 20
if(v.x > m.x) {pi <- m.x/v.x
beta <- m.x^2/(v.x-m.x) }
cat("\n", "Results for fitting the
negative binomial distribution", "\n")
cat("\n", "  Method of moment estimators ")
cat("\n", "     pi = ", pi)
cat("\n", "   beta = ", beta, "\n")
#  Compute maximum likelihood estimates
b <- matrix(c(pi, beta), nrow=2, ncol=1)
nbr <- mnr(b, x.dat, x.cat)
cat("\n", "  Maximum likelihood estimators")
cat("\n", "     pi = ", nbr$b[1])
cat("\n", "   beta = ", nbr$b[2], "\n")
cat("\n", "  Covariance matrix =", nbr$hi )
#  Compute expected counts for the
#  negative binomial model
m.nb <- sum(x.dat)*prob(nbr$b, x.dat, x.cat)
#  Combine categories in the right
#  tail of the distribution
nc <- length(x.dat)
xcc <- combine(x.cat, x.dat, m.nb, nc, mb)
# Compute the Pearson chi-squared
# test and p-value
x2p <- sum(((xcc$xt-xcc$ept)^2)/xcc$ept)
dfp <- length(xcc$ept) - 3
pvalp <- 1 - pchisq(x2p, dfp)
# Compute the G^2 statistic
g2<-0
for(i in 1:length(xcc$ept)) {
at<-0
if(xcc$xt[i] > 0)
{at<-2*xcc$xt[i]*log(xcc$xt[i]/xcc$ept[i])}
g2 <- g2+at}
pvalg <- 1-pchisq(g2, dfp)
#  print results
kk <- length(xcc$ept)
new3 <- matrix(c(xcc$cl, xcc$cu, xcc$xt, xcc$ept), kk, 4)
cat("\n", "  Results for fitting the Negative Binomial distribution", "\n")
cat("\n", "Category Bounds   Count   Expected", "\n")
print(new3)
cat("\n", "         Pearson test = ", x2p)
cat("\n", "   Degrees of freedom = ", dfp)
cat("\n", "              p-value = ", pvalp)
cat("\n", "Likelihood ratio test = ", g2)
cat("\n", "                   df = ", dfp)
cat("\n", "              p-value = " , pvalg)
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R')
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R', echo=TRUE)
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R')
?cumprod
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/rainevents.R')
comb
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/rainevents.R')
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R')
pi
pi <- nbr$b[1]
beta <- nbr$b[2]
invI <- nbr$hi
invI
mhat
mhat <- beta*(1-pi)/pi
mhat
D <- c(-beta/pi^2, (1-pi)/pi)
t(D)%*%invI%*%D
sqrt(0.03306799)
sigama <- sqrt(0.03306799)
mhat - 1.96*sigma
mhat - 1.96*sigama
mhat +1.96*sigama
log(2.93)+1.96*sqrt(1/67 + 1/34 + 1/43 + 1/64)
log(2.93) - 1.96*sqrt(1/67 + 1/34 + 1/43 + 1/64)
exp(0.509591)
exp(1.640414)
fisher.test(matrix(c(21,2,15,3),ncol=2, byrow=T))
library(vcd)
install.packages("vcd")
library(vcd)
x <- matrix(c(48, 26, 22, 8, 2, 4,
6, 4), nrow = 2, byrow=T)
assocstats(x)
assocstats(t(x))
xm<- matrix(c(113, 163, 370, 45, 106, 280,229, 343, 568), nrow = 3, byrow=T)
assocstats(xf)
xf<- matrix(c(100, 109, 202, 33, 89, 179,100, 179, 542), nrow = 3, byrow=T)
assocstats(xf)
assocstats(xm)
?fisher.test
fisher.test
x
fisher.test(x, simulate.p.value = T, B = 50000)
assocstats(xf)
assocstats(xm)
x
fisher.test(x)
Gamma2.f<-function(x, pr=0.95)
{
# x is a matrix of counts. You can use output of xtabs in R.
# Confidence interval calculation and output from Greg Rodd
# Check for using S-PLUS and output is from crosstabs (needs >= S-PLUS 6.0)
if(is.null(version$language) && inherits(x, "crosstabs")) { oldClass(x)<-NULL;
attr(x, "marginals")<-NULL}
n <- nrow(x)
m <- ncol(x)
pi.c<-pi.d<-matrix(0,nr=n,nc=m)
row.x<-row(x)
col.x<-col(x)
for(i in 1:(n)){
for(j in 1:(m)){
pi.c[i, j]<-sum(x[row.x<i & col.x<j]) + sum(x[row.x>i & col.x>j])
pi.d[i, j]<-sum(x[row.x<i & col.x>j]) + sum(x[row.x>i & col.x<j])}}
C <- sum(pi.c*x)/2
D <- sum(pi.d*x)/2
psi<-2*(D*pi.c-C*pi.d)/(C+D)^2
sigma2<-sum(x*psi^2)-sum(x*psi)^2
gamma <- (C - D)/(C + D)
pr2 <- 1 - (1 - pr)/2
CIa <- qnorm(pr2) * sqrt(sigma2) * c(-1, 1) + gamma
list(gamma = gamma, C = C, D = D, sigma = sqrt(sigma2), Level = paste(
100 * pr, "%", sep = ""), CI = paste(c("[", max(CIa[1], -1),
", ", min(CIa[2], 1), "]"), collapse = ""))
}
Gamma2.f(xf)
Gamma2.f(xm)
xb <- matrix(c(10, 5, 10, 31, 10, 4,
22, 18, 9), 3, 3, byrow=T)
xw <- matrix(c(9, 10, 5, 26, 6, 13,
10, 17, 10), 3, 3, byrow=T)
#   This file contains R code for computing a
#   Kappa statistic, or weighted Kappa statistic,
#   standard errors and confidence intervals.
#   It is applied to the student teacher data.
#   The file is posted as  kappa.R
# First create a function to compute kappa
kappa.f <- function(x, w=NULL)
{
#  Compute expected counts for random agreement
n <- sum(x)
xr <- apply(x, 1, sum)
xc <- apply(x, 2, sum)
one <- rep(1, length(xr))
e <- outer(xr, xc)/n
#  Compute the unweighted Kappa
k1 <- sum(diag(x))/n
k2 <- sum(diag(e))/n
k3 <- sum(diag(x)*diag(xr+xc))/(n*n)
k4 <- sum(((outer(xc, one)+
outer(one, xr))**2)*x)/(n**3)
kappa <- (k1-k2)/(1-k2)
#  Compute standard errors:
#     s1 does not assume random agreement
#     s2 assumes only random agreement
s11 <- (k1*(1-k1)/((1-k2)**2)+2*(1-k1)*
(2*k1*k2-k3)/((1-k2)**3)+
((1-k1)**2)*(k4-4*k2*k2)/((1-k2)**4))/n
s1 <- s11**.5
s22 <- (k2+k2*k2-(sum(diag(e)*diag(xr+xc))
/(n**2)))/(n*(1-k2)**2)
s2 <- s22**.5
# Compute default weights if no weights are provided
k <- dim(x)[2]
if (is.null(w)) {
w <- matrix(0, ncol = k, nrow = k)
for (i in 1:k) {
for (j in 1:k) {
w[i, j] <- 1 - (abs(i - j))^2/(k-1)^2
}
}
}
#  Compute the weighted Kappa
xw <- x*w
ew <- e*w
wr <- apply(w*xc, 2, sum)/n
wc <- apply(w*xr, 2, sum)/n
kw1 <- sum(xw)/n
kw2 <- sum(ew)/n
tt2 <- outer(wr, one)+outer(one, wc)
tt3 <- ((w*(1-kw2))-(tt2*(1-kw1)))**2
kappaw <- (kw1-kw2)/(1-kw2)
#  Compute standard errors:
#     sw11 does not assume random agreement
#     sw22  assumes only random agreement
sw11 <- sum(x*tt3)/n
sw11 <- (sw11-(kw1*kw2-2*kw2+kw1)**2)/
(n*(1-kw2)**4)
sw1 <- sw11**.5
sw22 <- (w-tt2)**2
sw22 <- ((sum(e*sw22)/n)-(kw2**2))/
(n*(1-kw2)**2)
sw2 <- sw22**.5
#  Construct 95% confidence intervals
#  and tests for random agreement
tk <- kappa/s2
tkw <- kappaw/sw2
tt4 <- tk**2
pk <- (1-pchisq(tt4, 1))
tt4 <- tkw**2
pkw <-(1-pchisq(tt4, 1))
ckl <- kappa-(1.96)*s1
cku <- kappa+(1.96)*s1
ckwl <- kappaw-(1.96)*sw1
ckwu <- kappaw+(1.96)*sw1
ww <- matrix( w, ncol=k, nrow=k)
#print results
cat("\n", "       Unweighted Kappa = ", signif(kappa,5))
cat("\n", "         Standard error = ", signif(s1,5))
cat("\n", "95% confidence interval:  ", signif(ckl,5), signif(cku,5))
cat("\n", "p-value for test of random agreement = ", signif(pk,5), "\n", "\n")
cat("\n", "         Weighted Kappa = ", signif(kappaw,5))
cat("\n", "         Standard error = ", signif(sw1,5))
cat("\n", "95% confidence interval:  ", signif(ckwl,5), signif(ckwu,5))
cat("\n", "p-value for test of random agreement = ", signif(pkw,5),"\n")
}
kappa.f(xw)
kappa.f(xb)
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/kappaboot.R')
results
boot.ci(results, type="all", index=1)
?boot.ci
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/kappaboot.R')
boot.ci(results, type="all", index=1)
0.061583^2 +  0.054328^2
-0.12456 - (-0.073792)
-0.050768 - 1.96 *0.006743997
-0.050768 + 1.96 *0.006743997
x.array <- array(c(81, 34, 24, 71, 118, 74, 69, 105, 82, 63, 52, 93),c(2,2,3))
mantelhaen.test(x.array, conf.level=0.95)
?mantelhaen.test
library(DescTools)
install.packages("DescTools")
library(DescTools)
BreslowDayTest(x.array)
mantelhaen.test(x.array, conf.level=0.95, exact = T)
?Zelen
??zelen
install.packages("NSM3")
library(NSM3)
zelen.test(x.array)
?zelen.test
zelen.test(example = T)
zelen.test(x.array, 5)
zelen.test(x.array, r = 5)
library(stats4)
minuslogl <- function(beta1, beta2, beta3, sigma2){
mu <- beta1*exp(-exp(beta2 - beta3*x))
v <- sigma2 * mu
l <- sum(log(2*pi*v)+(y - mu)^2/v)
return(l)
}
o1 <- mle(minuslogl = minuslogl, start = list(beta1 = 20, beta2 = 3, beta3 = 0.5, sigma2 = 0.2))
setwd("c:/Users/fanne/Dropbox/Homework/STAT520/STAT520hw5YifanZhu/")
d <- read.table("gompertzdat.txt", header = T)
attach(d)
o1 <- mle(minuslogl = minuslogl, start = list(beta1 = 20, beta2 = 3, beta3 = 0.5, sigma2 = 0.2))
D <- function(beta){
Dbeta <- rep(0,3)
Dbeta[1] <- 0
Dbeta[2] <- 1/beta[3]
Dbeta[3] <- -beta[2]/(beta[3])^2
return(Dbeta)
}
o1
?mle
coef(o1)
sdb2overb3 <- D(coef(o1))
db2overb3 <- D(coef(o1))
sdb2overb3 <- db2overb3%*%vcov(o1)[1:3,1:3]%*%t(db2overb3)
vcov(o1)[1:3,1:3]
db2overb3
dim(db2overb3)
db2overb3 <- matrix(db2overb3, nrow = 1, byrow = T)
sdb2overb3 <- db2overb3%*%vcov(o1)[1:3,1:3]%*%t(db2overb3)
coef(o1)$beta2
coef(o1)[beta2]
coef(o1)[2]
b2overb3est <- coef(o1)[2]/coef(o1)[3]
sdb2overb3 <- sqrt(db2overb3%*%vcov(o1)[1:3,1:3]%*%t(db2overb3))
b2overb3ci <- c(b2overb3est - 1.96 * sdb2overb3, b2overb3est + 1.96 * sdb2overb3)
b2overb3ci
b2overb3est
confint(o1)
coef(o1) - 1.96 * diag(sqrt(vcov(o1)))
betaI <- vcov(o1)[1:3, 1:3]
betaI
coef(o1)[1:3] - 1.96 * sqrt(diag(betaI))
coef(o1)
confint(o1)
coef(o1)[1:3] - 1.96 * sqrt(diag(betaI))
coef(o1)[1:3] + 1.96 * sqrt(diag(betaI))
coef(o1)
minuslogl2 <- function(p){
mu <- p[1]*exp(-exp(p[2] - p[3]*x))
v <- p[4] * mu
l <- sum(log(2*pi*v)+(y - mu)^2/v)
return(l)
}
o3 <- nlm(f = minuslogl2, p = c(20,3,0.5,0.2))
o3 <- nlm(f = minuslogl2, p = c(20,3,0.5,0.2), hessian = TRUE)
?nlm
o3$estimate
o3$gradient
o3$hessian
covb <- solve(o3$hessian)
cib
cib <-cbind(o3$estimate - 1.96 * sqrt(diag(covb)),o3$estimate + 1.96 * sqrt(diag(covb)))
cib
b2b3est2 <- o3$estimate[2]/o3$estimate[3]
db2b32 <- D(o3$estimate)
db2b32
sdb2b32 <- sqrt(t(db2b32)%*%covb[1:3, 1:3]%*%db2b32)
b2b3ci2 <- c(b2b3est2 - 1.96 * sdb2b32,b2b3est2 + 1.96 * sdb2b32 )
b2b3ci2
o3$gradient
y
x
derlogl <- function(p){
mu <- p[1]*exp(-exp(p[2] - p[3]*x))
v <- p[4]*mu
c <- (y - mu)/v + ((y - mu)^2/v - 1)/(2*mu)
T1 <- exp(-exp(p[2] - p[3]*x))
T2 <- exp(p[2] - p[3]*x)
mubeta1 <- T1
mubeta2 <- -p[1]*T1*T2
mubeta3 <- p[1]*x*T1*T2
lv <- (1/2*v)*((y - mu)^2/v - 1)
return(c(c*mubeta1, c*mubeta2, c*mubeta3, lv*mu))
}
source("nonlin.r")
#---------------------------------------------------------
gompfctn<-function(xs,ps){
#Gompertz response curve
#ps is (b1,b2,b3)
b1<-ps[1]; b2<-ps[2]; b3<-ps[3]
fs<-b1*exp(-exp(b2-b3*xs))
return(fs)
}
#--------------------------------------------------------
gompVmat<-function(xs,ps){
#compute matrix of derivatives for Gompertz model for use with nonlin
#
n<-length(xs)
b1<-ps[1]; b2<-ps[2]; b3<-ps[3]
t1<-exp(b2-b3*xs)
t2<-exp((-1)*t1)
db1<-t2
db2<-(-1)*b1*t2*t1
db3<-b1*t2*t1*xs
V<-matrix(c(db1,db2,db3),n,3,byrow=F)
return(V)
}
#------------------------------------------------------------
gompwts<-function(xs,ps){
4
#weights for gompertz model with power of the mean variances
#power (thet) must be changed within this function
#
thet<-0.50
mus<-gompfctn(xs,ps)
ws<-1/(mus^(2*thet))
W<-diag(ws)
return(W)
}
#--------------------------------------------------------------
o <- nonlin(xmat = x, ys = y, ps = c(20,3,0.5), fctn = gompfctn, ders = gompVmat, wts = gompwts)
derlogl(o$bs)
derlogl <- function(p){
mu <- p[1]*exp(-exp(p[2] - p[3]*x))
v <- p[4]*mu
c <- (y - mu)/v + ((y - mu)^2/v - 1)/(2*mu)
T1 <- exp(-exp(p[2] - p[3]*x))
T2 <- exp(p[2] - p[3]*x)
mubeta1 <- T1
mubeta2 <- -p[1]*T1*T2
mubeta3 <- p[1]*x*T1*T2
lv <- (1/2*v)*((y - mu)^2/v - 1)
return(c(sum(c*mubeta1), sum(c*mubeta2), sum(c*mubeta3), sum(lv*mu)))
}
derlogl(o$bs)
derlogl(c(o$bs, o$sshat))
derlogl(o3$estimate)
coef(o2)
coef(o1)
derlogl(coef(o1))
o3$gradient
minuslogl1 <- function(beta1, beta2, beta3, sigma2, theta){
mu <- beta1*exp(-exp(beta2 - beta3*x))
v <- sigma2 * mu^(2*theta)
l <- sum(log(2*pi*v)+(y - mu)^2/v)
return(l)
}
o4 <- mle(minuslogl = minuslogl1, start = list(beta1 = 20, beta2 = 3, beta3 = 0.5, sigma2 = 0.2, theta = 0.6) )
warning(o4)
o4
confint(o4)
warnings()
logLik(o4)
-qchisq(p=0.95, df = 1)/2 + logLik(o4)
estp <- coef(o4)
minuslogl1(beta1 = estp[1], beta2 = estp[2], beta3 = estp[3], sigma2 = estp[4], theta = estp[5])
minuslogl1(beta1 = estp[1], beta2 = estp[2], beta3 = estp[3], sigma2 = estp[4], theta = 0.49720086)
minuslogl1(beta1 = estp[1], beta2 = estp[2], beta3 = estp[3], sigma2 = estp[4], theta = 0.437)
o5 <- mle(minuslogl = minuslogl1, start = list(beta1 = 20, beta2 = 3, beta3 = 0.5, sigma2 = 0.2), fixed = list(theta = 0.437))
logLik(o5)
o5 <- mle(minuslogl = minuslogl1, start = list(beta1 = 20, beta2 = 3, beta3 = 0.5, sigma2 = 0.2), fixed = list(theta = 0.497))
logLik(o5)
ci5 <- cbind(coef(o4) - 1.96 * sqrt(diag(vcov(o4))),coef(o4) + 1.96 * sqrt(diag(vcov(o4))) )
ci5
confint(o4)
