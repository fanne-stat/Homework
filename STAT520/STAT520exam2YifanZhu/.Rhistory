#   xt -- the observed count for a combined
#             category.
#  ept -- the expected count for a combined
#             category.
combine<-function(cc, x, ep, nc, mb)
{
ptr <- nc
I <- c()
k <- 0
cl<-cc
cu<-cc
while(ptr > 1) {
ptrm1 <- ptr - 1
if(ep[ptr] < mb) {
ep[ptrm1] <- ep[ptrm1] + ep[ptr]
x[ptrm1] <- x[ptrm1] + x[ptr]
cu[ptrm1]<-cu[ptr]
}
else {
k <- k + 1
I[k] <- ptr
}
ptr <- ptrm1
}
if(ep[1] < mb) {
Ik <- I[k]
ep[Ik] <- ep[Ik] + ep[1]
x[Ik] <- x[Ik] + x[1]
cl[Ik]<-cl[1]
}
else {
k <- k + 1
I[k] <- 1
}
II <- I[k:1]
list(k=k, cl = cl[II], cu=cu[II],
xt = x[II], ept = ep[II])
}
#  Function for evaluating the
#  log-likelihood function
fun <- function(b, x.dat, x.cat)
{
if(b[2,1] > 40) {stop(paste("beta > 40"))}
else
{f <- crossprod(x.dat, lgamma(x.cat+b[2,1])-lgamma(x.cat+1))
f <- f +  sum(x.dat)*(b[2,1]* log(b[1,1])- lgamma(b[2,1]))
f <- f + crossprod(x.dat, x.cat)*log(1-b[1,1])}
f
}
#  Function for evaluating negative
#  binomial probabilities
prob <- function(b, x.dat, x.cat)
{
pp <- exp(lgamma(x.cat+b[2,1])
-lgamma(x.cat+1)-lgamma(b[2,1])
+(b[2,1]*log(b[1,1]))
+x.cat*log(1-b[1,1]))
kt <- length(pp)
pp[kt] <- 1 - sum(pp) + pp[kt]
pp
}
#  Function for computing first
#  partial derivatives and the
#  negative of the matrix of the
#  matrix of second partial derivatives
#  of the negative binomial log-likelihood
dfun <- function(b, x.dat, x.cat)
{
xcum <- sum(x.dat)-cumsum(x.dat)+x.dat
q <- matrix(c(1,1), 2, 1)
h <- matrix(c(1, 1, 1, 1), 2, 2)
q[1,1] <- sum(x.dat)*b[2,1]/b[1,1] -
crossprod(x.dat, x.cat)/(1-b[1,1])
q[2,1] <- sum(xcum/(x.cat+(b[2,1]-1)))-
(xcum[1]/(b[2,1]-1)) +
(sum(x.dat)*log(b[1,1]))
h[1,1] <- sum(x.dat)*b[2,1]/(b[1,1]^2) +
crossprod(x.dat, x.cat)/((1-b[1,1])^2)
h[1,2] <- -sum(x.dat)/b[1,1]
h[2,1] <- h[1,2]
h[2,2] <- sum(xcum/((x.cat+(b[2,1]-1))^2))
h[2,2] <- h[2,2]-(xcum[1]/((b[2,1]-1)^2))
list(q=q, h=h)
}
# Modified Newton-Raphson Algorithm
mnr <- function(b, x.dat, x.cat,
maxit = 50, halving = 16,
conv = .000001)
{
check <- 1
iter <- 1
while(check > conv && iter < maxit+1) {
fold <- fun(b, x.dat, x.cat)
bold <- b
aa <- dfun(b, x.dat, x.cat)
hi <- solve(aa$h)
b <- bold + hi%*%aa$q
fnew <- fun(b, x.dat, x.cat)
hiter <- 1
while(fold-fnew > 0 && hiter < halving+1) {
b <- bold + 2.*((.5)^hiter)*hi%*%aa$q
fnew <- fun(b, x.dat, x.cat)
hiter <- hiter + 1
}
cat("\n", "Iteration = ", iter, " pi =", b[1,1],
" beta = ", b[2,1], "Log-likelihood", fnew)
iter <- iter + 1
check <- crossprod(bold-b,bold-b)/crossprod(bold,bold)
}
aa <- dfun(b, x.dat, x.cat)
hi <- solve(aa$h)
list(b = b, hi = hi, grad = aa$q)
}
#  The data in this example are
#  the smooth surface cavity data
#  considered in STAT 557.
#  Enter a list of counts for each
#  of the categories from zero
#  through K, the maximum number of
#  cavities seen in any one child.
#  All categories preceeding K must
#  be included, even if the observed
#  count is zero. There should be K+1
#  entries in the list of counts.
x.dat <- c(57,203,383,525,532,408,273,139,45,27,10,4,0,1,1)
mb<-2
#  Enter the category values
nc <- length(x.dat)
x.cat <- 0:(nc-1)
#  Compute the total count, mean,
#  and variance
n <- sum(x.dat)
m.x <- crossprod(x.dat, x.cat)/n
v.x <- (crossprod(x.dat, x.cat^2) - n*m.x^2)/n
#  Compute expected counts for
#  the Poisson model
m.p <- (n/m.x)*exp(-m.x)*
cumprod(m.x*(c(1,1:(length(x.dat)-1))^(-1)))
# Combine categories to keep expected
# counts above a lower bound
xcc <- combine(x.cat, x.dat, m.p, nc, mb)
# Compute the Pearson chi-squared
# test and p-value
x2p <- sum((xcc$xt-xcc$ept)^2/xcc$ept)
dfp <- length(xcc$ept) - 2
pvalp <- 1 - pchisq(x2p, dfp)
# Compute the G^2 statistic
g2<-0
for(i in 1:length(xcc$ept)) {
at<-0
if(xcc$xt[i] > 0)
{at<-2*xcc$xt[i]*log(xcc$xt[i]/xcc$ept[i])}
g2 <- g2+at}
pvalg <- 1 - pchisq(g2, dfp)
# Compute the Fisher Index of Dispersion test statistic
# The one-sided alternative is that the variance is
# larger than the mean.
fisherd <- n*v.x/m.x
dff <- n -1
pvalf <- 1 - pchisq(fisherd, dff)
# Print results
kk <- length(xcc$ept)
new2 <- matrix(c(xcc$cl, xcc$cu, xcc$xt, xcc$ept),
kk, 4)
cat("\n", "  Results for fitting the
Poisson distribution", "\n")
cat("\n", "Category Bounds  Count  Expected", "\n")
print(new2)
cat("\n", "      Pearson test = ", x2p)
cat("\n", "Degrees of freedom = ", dfp)
cat("\n", "           p-value = ", pvalp, "\n")
cat("   Likelihood ratio test =", g2, "\n")
cat("                      df =", dfp, "\n")
cat("                  p-value =", pvalg, "\n")
cat("\n","Fisher Index of Dispersion = ", fisherd)
cat("\n", "    Degrees of freedom = ", dff)
cat("\n", "               p-value = ", pvalf )
#  Begin computation for fitting the
#  negative binomial distribution
pi <- .95
beta <- 20
if(v.x > m.x) {pi <- m.x/v.x
beta <- m.x^2/(v.x-m.x) }
cat("\n", "Results for fitting the
negative binomial distribution", "\n")
cat("\n", "  Method of moment estimators ")
cat("\n", "     pi = ", pi)
cat("\n", "   beta = ", beta, "\n")
#  Compute maximum likelihood estimates
b <- matrix(c(pi, beta), nrow=2, ncol=1)
nbr <- mnr(b, x.dat, x.cat)
cat("\n", "  Maximum likelihood estimators")
cat("\n", "     pi = ", nbr$b[1])
cat("\n", "   beta = ", nbr$b[2], "\n")
cat("\n", "  Covariance matrix =", nbr$hi )
#  Compute expected counts for the
#  negative binomial model
m.nb <- sum(x.dat)*prob(nbr$b, x.dat, x.cat)
#  Combine categories in the right
#  tail of the distribution
nc <- length(x.dat)
xcc <- combine(x.cat, x.dat, m.nb, nc, mb)
# Compute the Pearson chi-squared
# test and p-value
x2p <- sum(((xcc$xt-xcc$ept)^2)/xcc$ept)
dfp <- length(xcc$ept) - 3
pvalp <- 1 - pchisq(x2p, dfp)
# Compute the G^2 statistic
g2<-0
for(i in 1:length(xcc$ept)) {
at<-0
if(xcc$xt[i] > 0)
{at<-2*xcc$xt[i]*log(xcc$xt[i]/xcc$ept[i])}
g2 <- g2+at}
pvalg <- 1-pchisq(g2, dfp)
#  print results
kk <- length(xcc$ept)
new3 <- matrix(c(xcc$cl, xcc$cu, xcc$xt, xcc$ept), kk, 4)
cat("\n", "  Results for fitting the Negative Binomial distribution", "\n")
cat("\n", "Category Bounds   Count   Expected", "\n")
print(new3)
cat("\n", "         Pearson test = ", x2p)
cat("\n", "   Degrees of freedom = ", dfp)
cat("\n", "              p-value = ", pvalp)
cat("\n", "Likelihood ratio test = ", g2)
cat("\n", "                   df = ", dfp)
cat("\n", "              p-value = " , pvalg)
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R')
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R', echo=TRUE)
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R')
?cumprod
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/rainevents.R')
comb
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/rainevents.R')
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/negbin(2).R')
pi
pi <- nbr$b[1]
beta <- nbr$b[2]
invI <- nbr$hi
invI
mhat
mhat <- beta*(1-pi)/pi
mhat
D <- c(-beta/pi^2, (1-pi)/pi)
t(D)%*%invI%*%D
sqrt(0.03306799)
sigama <- sqrt(0.03306799)
mhat - 1.96*sigma
mhat - 1.96*sigama
mhat +1.96*sigama
log(2.93)+1.96*sqrt(1/67 + 1/34 + 1/43 + 1/64)
log(2.93) - 1.96*sqrt(1/67 + 1/34 + 1/43 + 1/64)
exp(0.509591)
exp(1.640414)
fisher.test(matrix(c(21,2,15,3),ncol=2, byrow=T))
library(vcd)
install.packages("vcd")
library(vcd)
x <- matrix(c(48, 26, 22, 8, 2, 4,
6, 4), nrow = 2, byrow=T)
assocstats(x)
assocstats(t(x))
xm<- matrix(c(113, 163, 370, 45, 106, 280,229, 343, 568), nrow = 3, byrow=T)
assocstats(xf)
xf<- matrix(c(100, 109, 202, 33, 89, 179,100, 179, 542), nrow = 3, byrow=T)
assocstats(xf)
assocstats(xm)
?fisher.test
fisher.test
x
fisher.test(x, simulate.p.value = T, B = 50000)
assocstats(xf)
assocstats(xm)
x
fisher.test(x)
Gamma2.f<-function(x, pr=0.95)
{
# x is a matrix of counts. You can use output of xtabs in R.
# Confidence interval calculation and output from Greg Rodd
# Check for using S-PLUS and output is from crosstabs (needs >= S-PLUS 6.0)
if(is.null(version$language) && inherits(x, "crosstabs")) { oldClass(x)<-NULL;
attr(x, "marginals")<-NULL}
n <- nrow(x)
m <- ncol(x)
pi.c<-pi.d<-matrix(0,nr=n,nc=m)
row.x<-row(x)
col.x<-col(x)
for(i in 1:(n)){
for(j in 1:(m)){
pi.c[i, j]<-sum(x[row.x<i & col.x<j]) + sum(x[row.x>i & col.x>j])
pi.d[i, j]<-sum(x[row.x<i & col.x>j]) + sum(x[row.x>i & col.x<j])}}
C <- sum(pi.c*x)/2
D <- sum(pi.d*x)/2
psi<-2*(D*pi.c-C*pi.d)/(C+D)^2
sigma2<-sum(x*psi^2)-sum(x*psi)^2
gamma <- (C - D)/(C + D)
pr2 <- 1 - (1 - pr)/2
CIa <- qnorm(pr2) * sqrt(sigma2) * c(-1, 1) + gamma
list(gamma = gamma, C = C, D = D, sigma = sqrt(sigma2), Level = paste(
100 * pr, "%", sep = ""), CI = paste(c("[", max(CIa[1], -1),
", ", min(CIa[2], 1), "]"), collapse = ""))
}
Gamma2.f(xf)
Gamma2.f(xm)
xb <- matrix(c(10, 5, 10, 31, 10, 4,
22, 18, 9), 3, 3, byrow=T)
xw <- matrix(c(9, 10, 5, 26, 6, 13,
10, 17, 10), 3, 3, byrow=T)
#   This file contains R code for computing a
#   Kappa statistic, or weighted Kappa statistic,
#   standard errors and confidence intervals.
#   It is applied to the student teacher data.
#   The file is posted as  kappa.R
# First create a function to compute kappa
kappa.f <- function(x, w=NULL)
{
#  Compute expected counts for random agreement
n <- sum(x)
xr <- apply(x, 1, sum)
xc <- apply(x, 2, sum)
one <- rep(1, length(xr))
e <- outer(xr, xc)/n
#  Compute the unweighted Kappa
k1 <- sum(diag(x))/n
k2 <- sum(diag(e))/n
k3 <- sum(diag(x)*diag(xr+xc))/(n*n)
k4 <- sum(((outer(xc, one)+
outer(one, xr))**2)*x)/(n**3)
kappa <- (k1-k2)/(1-k2)
#  Compute standard errors:
#     s1 does not assume random agreement
#     s2 assumes only random agreement
s11 <- (k1*(1-k1)/((1-k2)**2)+2*(1-k1)*
(2*k1*k2-k3)/((1-k2)**3)+
((1-k1)**2)*(k4-4*k2*k2)/((1-k2)**4))/n
s1 <- s11**.5
s22 <- (k2+k2*k2-(sum(diag(e)*diag(xr+xc))
/(n**2)))/(n*(1-k2)**2)
s2 <- s22**.5
# Compute default weights if no weights are provided
k <- dim(x)[2]
if (is.null(w)) {
w <- matrix(0, ncol = k, nrow = k)
for (i in 1:k) {
for (j in 1:k) {
w[i, j] <- 1 - (abs(i - j))^2/(k-1)^2
}
}
}
#  Compute the weighted Kappa
xw <- x*w
ew <- e*w
wr <- apply(w*xc, 2, sum)/n
wc <- apply(w*xr, 2, sum)/n
kw1 <- sum(xw)/n
kw2 <- sum(ew)/n
tt2 <- outer(wr, one)+outer(one, wc)
tt3 <- ((w*(1-kw2))-(tt2*(1-kw1)))**2
kappaw <- (kw1-kw2)/(1-kw2)
#  Compute standard errors:
#     sw11 does not assume random agreement
#     sw22  assumes only random agreement
sw11 <- sum(x*tt3)/n
sw11 <- (sw11-(kw1*kw2-2*kw2+kw1)**2)/
(n*(1-kw2)**4)
sw1 <- sw11**.5
sw22 <- (w-tt2)**2
sw22 <- ((sum(e*sw22)/n)-(kw2**2))/
(n*(1-kw2)**2)
sw2 <- sw22**.5
#  Construct 95% confidence intervals
#  and tests for random agreement
tk <- kappa/s2
tkw <- kappaw/sw2
tt4 <- tk**2
pk <- (1-pchisq(tt4, 1))
tt4 <- tkw**2
pkw <-(1-pchisq(tt4, 1))
ckl <- kappa-(1.96)*s1
cku <- kappa+(1.96)*s1
ckwl <- kappaw-(1.96)*sw1
ckwu <- kappaw+(1.96)*sw1
ww <- matrix( w, ncol=k, nrow=k)
#print results
cat("\n", "       Unweighted Kappa = ", signif(kappa,5))
cat("\n", "         Standard error = ", signif(s1,5))
cat("\n", "95% confidence interval:  ", signif(ckl,5), signif(cku,5))
cat("\n", "p-value for test of random agreement = ", signif(pk,5), "\n", "\n")
cat("\n", "         Weighted Kappa = ", signif(kappaw,5))
cat("\n", "         Standard error = ", signif(sw1,5))
cat("\n", "95% confidence interval:  ", signif(ckwl,5), signif(ckwu,5))
cat("\n", "p-value for test of random agreement = ", signif(pkw,5),"\n")
}
kappa.f(xw)
kappa.f(xb)
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/kappaboot.R')
results
boot.ci(results, type="all", index=1)
?boot.ci
source('C:/Users/fanne/Dropbox/Homework/STAT557/STAT557hw2YifanZhu/kappaboot.R')
boot.ci(results, type="all", index=1)
0.061583^2 +  0.054328^2
-0.12456 - (-0.073792)
-0.050768 - 1.96 *0.006743997
-0.050768 + 1.96 *0.006743997
x.array <- array(c(81, 34, 24, 71, 118, 74, 69, 105, 82, 63, 52, 93),c(2,2,3))
mantelhaen.test(x.array, conf.level=0.95)
?mantelhaen.test
library(DescTools)
install.packages("DescTools")
library(DescTools)
BreslowDayTest(x.array)
mantelhaen.test(x.array, conf.level=0.95, exact = T)
?Zelen
??zelen
install.packages("NSM3")
library(NSM3)
zelen.test(x.array)
?zelen.test
zelen.test(example = T)
zelen.test(x.array, 5)
zelen.test(x.array, r = 5)
install.packages("VGAM")
M <- c(500, 1000*1:10, 20000, 50000, 100000, 200000)
M
?boot
library(boot)
?boot
aircondit
d <- data.frame(n = 10, lambda = 5, method1 = c(1,2,3))
d
?row.names
row.names(d) <- c("m1, m2, m3")
row.names(d) <- c("", m1, m2, m3")
)
row.names(d)
row.names(d) <- c("m1", "m2", "m3")
d
rm(d)
n = 10
mu = 5
lambda = 2
library(statmod)
y1 <- rinvgauss(n, mean = mu, shape = lambda)
source("./functions.R")
thetamle <- function(y){
muhat <- sum(y)/n
lambdahat <- 1/(sum(1/y)/n - 1/muhat)
return(c(muhat, lambdahat))
}
vthetamle <- function(thetahat){
vmu <- thetahat[1]^3/(thetahat[2]*n)
vlambda <- 2*thetahat[2]^2/n
return(c(vmu, vlambda))
}
muwaldint <- function(muhat, vmu){
c(muhat - 1.96*sqrt(vmu), muhat + 1.96*sqrt(vmu))
}
lambdawaldint <- function(lambdahat, vlambda){
c(lambda - 1.96*sqrt(vlambda), lambdahat + 1.96*sqrt(vlambda))
}
loglike <- function(mu, lambda, y){
n*log(lambda)/2 - sum(y)*lambda/(2*mu^2) + lambda*n/mu - sum(1/y)*lambda/2
}
mulrt <- function(mu, thetahat, y){
lambdatil <- 1/(sum(y)/(n*mu^2) + sum(1/y)/n - 2/mu)
return(loglike(thetahat[1], thetahat[2], y) - loglike(mu, lambdatil, y) - 1.92)
}
lambdalrt <- function(lambda, thetahat, y){
mutil <- sum(y)/n
return(loglike(thetahat[1], thetahat[2], y) - loglike(mutil, lambda, y) - 1.92)
}
muinvlikint <- function(y, thetahat, muwald){
lower <- uniroot(function(mu) mulrt(mu, thetahat, y), c(0.01, thetahat[1]))
upper <- uniroot(function(mu) mulrt(mu, thetahat, y), c(thetahat[1], 2*muwald[2]))
return(c(lower$root, upper$root))
}
lambdainvlikint <- function(y, thetahat, lambdawald){
lower <- uniroot(function(lambda) lambdalrt(lambda, y = y, thetahat = thetahat), c(0.01, thetahat[2]))
upper <- uniroot(function(lambda) lambdalrt(lambda, y = y, thetahat = thetahat), c(thetahat[2], 10*lambdawald[2]))
return(c(lower$root, upper$root))
}
thetahat1 <- thetamle(y1)
x1 <- seq(0.01, 100, 0.1)
lapply(x1, function(mu) mulrt(mu, thetahat1, y1))
sapply(x1, function(mu) mulrt(mu, thetahat1, y1))
plot(x1, sapply(x1, function(mu) mulrt(mu, thetahat1, y1)))
plot(x1, sapply(x1, function(mu) lambdalrt(mu, thetahat1, y1)))
?tryCatch
uniroot(function(mu) mulrt(mu, thetahat, y)
)
uniroot(function(mu) mulrt(mu, thetahat1, y), c(0.01,10000))
tryCatch({uniroot(function(mu) mulrt(mu, thetahat1, y), c(0.01,10000))}, error = function(x) Inf)
Inf - a
Inf - 0.1
vthemle(thetahat1)
vthetamle <- function(thetahat){
vmu <- thetahat[1]^3/(thetahat[2]*n)
vlambda <- 2*thetahat[2]^2/n
return(c(vmu, vlambda))
}
vthetahat1 <- vthemle(thetahat1)
vthetahat1 <- vthetamle(thetahat1)
muwald1 <- muwaldint(thetahat1[1], vthetahat1[1])
tryCatch({uniroot(function(mu) mulrt(mu, thetahat, y), c(thetahat[1], 10*muwald[2]))}, error = function(x) Inf)
tryCatch({uniroot(function(mu) mulrt(mu, thetahat, y), c(thetahat[1], 10*muwald[2]))$root}, error = function(x) Inf)
uniroot(function(mu) mulrt(mu, thetahat, y), c(thetahat[1], 10*muwald[2]))$root
setwd('c:/Users/fanne/Dropbox/Homework/STAT520/STAT520exam2YifanZhu/')
source('C:/Users/fanne/Dropbox/Homework/STAT520/STAT520exam2YifanZhu/run.R')
source('C:/Users/fanne/Dropbox/Homework/STAT520/STAT520exam2YifanZhu/run.R')
source('C:/Users/fanne/Dropbox/Homework/STAT520/STAT520exam2YifanZhu/run.R')
source('C:/Users/fanne/Dropbox/Homework/STAT520/STAT520exam2YifanZhu/run.R')
